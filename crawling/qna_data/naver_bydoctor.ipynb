{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필수 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 모듈 임포트\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json 저장 및 로드 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "def read_from_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱스 호출 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_last_index(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data.get('last_completed_page') or data.get('last_processed_index')\n",
    "    except FileNotFoundError:\n",
    "        return 0  # 파일이 없을 경우 0부터 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 의사 정보 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_doctor_profiles(max_pages, start_page=0):\n",
    "    base_url = 'https://kin.naver.com/people/expert/index.naver?type=DOCTOR&page={}'\n",
    "    doctor_info = []\n",
    "\n",
    "    for page in tqdm(range(start_page, max_pages + 1)):\n",
    "        url = base_url.format(page)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 각 의사의 상세 페이지 링크와 정보를 찾아 리스트에 추가\n",
    "        for item in soup.select('.pro_list li'):\n",
    "            doctor_link_tag = item.find('h5').find('a')\n",
    "            if doctor_link_tag:\n",
    "                doctor_name = doctor_link_tag.text.strip()  # 닥터 이름 추출\n",
    "                doctor_id = doctor_link_tag['href'].split('u=')[1]  # 사용자 ID 추출\n",
    "                specialty_tag = item.find('h6')\n",
    "                specialty = specialty_tag.text.strip() if specialty_tag else '정보 없음' # 전문과목 추출\n",
    "                affiliation_tag = item.find('th', string='소속기관')\n",
    "                affiliation = affiliation_tag.find_next('td').text.strip() if affiliation_tag else '정보 없음' # 소속기관 추출\n",
    "                answer_count_tag = item.find('th', string='총 답변')\n",
    "                answer_count = int(answer_count_tag.find_next('td').text.strip().replace(',', '')) if answer_count_tag else 0 # 총 답변 수 추출\n",
    "\n",
    "                # 의사 정보를 딕셔너리로 저장\n",
    "                doctor_info.append({\n",
    "                    'doctor_id': doctor_id,\n",
    "                    'doctor_name': doctor_name,\n",
    "                    'specialty': specialty,\n",
    "                    'total_answers': answer_count,\n",
    "                    'affiliation': affiliation\n",
    "                })\n",
    "        save_to_json({'last_completed_page': page}, 'last_page.json')\n",
    "\n",
    "    return doctor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'doctor_id': 'gnsPR0eqbWOxYUkkRlwZHh4jTVTtCnsbB6TTnlCmF7A%3D',\n",
       " 'doctor_name': '문인희',\n",
       " 'specialty': '비전이비인후과 원장',\n",
       " 'total_answers': 3212,\n",
       " 'affiliation': '대한의사협회'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과값 예시\n",
    "doctor_profiles = scrape_doctor_profiles(1)\n",
    "doctor_profiles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 질문 id 크롤링(의사별)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info(doctor_id, total_answers):\n",
    "    max_pages = total_answers // 20 + 1 if total_answers % 20 else total_answers // 20\n",
    "    base_url = 'https://kin.naver.com/userinfo/expert/answerList.naver?u={user_id}&page={page}'\n",
    "    all_info = {}\n",
    "\n",
    "    for page in tqdm(range(1, max_pages + 1), desc=f\"Scraping {doctor_id}\"):\n",
    "        url = base_url.format(user_id=doctor_id, page=page)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # 원하는 정보를 추출하는 정규 표현식 패턴 정의\n",
    "        pattern = r'<a href=\"/qna/detail\\.naver\\?d1id=\\d+&dirId=\\d+&docId=(\\d+)\"'\n",
    "        # 정규 표현식으로 링크 찾기\n",
    "        matches = re.findall(pattern, response.text)\n",
    "        # 날짜 데이터를 추출하여 리스트로 저장\n",
    "        dates = [date.text.strip() for date in soup.select('.t_num.tc')]\n",
    "        # 찾아진 링크와 날짜를 all_info에 추가\n",
    "        for i in range(len(matches)):\n",
    "            doc_id = matches[i]\n",
    "            date = dates[i] if i < len(dates) else '날짜 없음'\n",
    "            if doctor_id not in all_info:\n",
    "                all_info[doctor_id] = []\n",
    "            all_info[doctor_id].append({'doc_id': doc_id, 'date': date})\n",
    "    return all_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_ids = scrape_info(doctor_profiles[0]['doctor_id'],doctor_profiles[0][\"total_answers\"])\n",
    "# doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 doc_id 추출 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_doc_ids(doctor_profiles, start_index=0):\n",
    "    doc_ids_data = {}\n",
    "    for i, profile in tqdm(enumerate(doctor_profiles[start_index:], start=start_index), total=len(doctor_profiles[start_index:])):\n",
    "        doctor_id = profile['doctor_id']\n",
    "        doc_ids = scrape_info(doctor_id, profile['total_answers'])\n",
    "        doc_ids_data[doctor_id] = doc_ids\n",
    "        save_to_json({'last_processed_index': i}, 'last_index.json')\n",
    "    return doc_ids_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_ids = scrape_doc_ids(doctor_profiles)\n",
    "# doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질문 답변 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크래핑 함수 정의\n",
    "def scrape_details(doc_ids):\n",
    "    base_url = 'https://kin.naver.com/qna/detail.naver?d1id=7&dirId=70201&docId={}'\n",
    "    head = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7,ja;q=0.6',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
    "    'Upgrade-Insecure-Requests': '1',}\n",
    "    retry_delays = [5, 10, 20]\n",
    "    \n",
    "    # 데이터를 저장할 빈 리스트 생성\n",
    "    title_list = []\n",
    "    question_list = []\n",
    "    answer_list = []\n",
    "\n",
    "    # tqdm을 사용하여 진행 상황을 시각화\n",
    "    for doc_id in tqdm(doc_ids):\n",
    "        attempt = 0\n",
    "        while attempt <= len(retry_delays):\n",
    "            try:\n",
    "                url = base_url.format(doc_id['doc_id'])\n",
    "                r = requests.get(url, headers=head)\n",
    "                r.raise_for_status()\n",
    "                bs = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "                title_data = bs.select_one('.title') # 질문 제목\n",
    "                title = title_data.text.strip() if title_data else None\n",
    "                \n",
    "                question_data = bs.select_one('.c-heading__content') # 질문 내용\n",
    "                question = question_data.text.strip() if question_data else None\n",
    "                \n",
    "                answer_data = bs.select_one('.se-main-container') # 답변\n",
    "                answer = answer_data.text.strip() if answer_data else None\n",
    "\n",
    "                title_list.append(title)\n",
    "                question_list.append(question)\n",
    "                answer_list.append(answer)\n",
    "                break\n",
    "\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if e.response.status_code == 429:  # Too Many Requests\n",
    "                    if attempt == len(retry_delays):\n",
    "                        tqdm.write(f\"Failed to scrape doc_id {doc_id['doc_id']} after multiple attempts.\")\n",
    "                        break  # 최대 재시도 횟수에 도달하면 실패로 간주\n",
    "                    tqdm.write(f\"Rate limit reached. Retrying in 1 seconds...\")\n",
    "                    time.sleep(1)  # 지정된 시간만큼 대기\n",
    "                    attempt += 1\n",
    "                else:\n",
    "                    tqdm.write(f\"Failed to scrape doc_id {doc_id['doc_id']}: {e}\")\n",
    "                    break  # 다른 유형의 HTTP 에러는 재시도하지 않음\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                tqdm.write(f\"Failed to scrape doc_id {doc_id['doc_id']}: {e}\")\n",
    "                break  # 네트워크 문제 등 다른 예외에 대한 처리\n",
    "            time.sleep(1)  # 다음 요청까지의 기본 지연 시간\n",
    "\n",
    "\n",
    "    data = {'title': title_list, 'question': question_list, 'answer': answer_list}\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⭐️모든 doc_id 추출⭐️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [03:35<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to all_doctor_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# all_doctor_profiles 추출\n",
    "max_pages = 526\n",
    "last_page = load_last_index('last_page.json')\n",
    "doctor_profiles = scrape_doctor_profiles(max_pages, start_page=last_page)\n",
    "\n",
    "# json 저장\n",
    "doctor_profiles_filename = \"all_doctor_profiles.json\"\n",
    "save_to_json(doctor_profiles, doctor_profiles_filename)\n",
    "print(f\"Data saved to {doctor_profiles_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_ids 추출\n",
    "last_index = load_last_index('last_index.json')\n",
    "doctor_profiles = read_from_json(\"all_doctor_profiles.json\")\n",
    "doc_ids_data = scrape_doc_ids(doctor_profiles, start_index=last_index)\n",
    "\n",
    "# json 저장\n",
    "json_filename = \"doc_ids_data.json\"\n",
    "save_to_json(doc_ids_data, json_filename)\n",
    "print(f\"Data saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의사 1명에 대한 샘플 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor_profiles = scrape_doctor_profiles(1)\n",
    "doctor_profiles[0], doctor_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_info = scrape_info(doctor_profiles[0]['doctor_id'],doctor_profiles[0]['total_answers'])\n",
    "doc_ids_info, doc_ids_info[doctor_profiles[0]['doctor_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_details(doc_ids_info[doctor_profiles[0]['doctor_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doctor in doctor_profiles:\n",
    "    doctor_id = doctor['doctor_id']\n",
    "    total_answers = doctor['total_answers']\n",
    "    doc_ids_info = scrape_info(doctor_id, total_answers)\n",
    "    if doctor_id in doc_ids_info:\n",
    "        df = scrape_details(doc_ids_info[doctor_id])\n",
    "        # 파일명에 의사 ID를 포함하여 각 의사별로 고유한 JSON 파일 생성\n",
    "        filename = f'doctor_data_{doctor_id}.json'\n",
    "        df.to_json(filename, orient='records', force_ascii=False, lines=True)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
