{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필수 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_7648\\3214259401.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# 필수 모듈 임포트\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from requests.exceptions import Timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_details 함수에서 timeout된 doc_ids를 저장할 list 생성\n",
    "\n",
    "timeout_doc_ids=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json 저장 및 로드 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    # 파일명에 디렉토리 경로가 포함되어 있지 않으면 현재 디렉토리를 사용\n",
    "    directory = os.path.dirname(filename) or '.'\n",
    "    os.makedirs(directory, exist_ok=True)  # 디렉토리 생성\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        \n",
    "def read_from_json(filename):\n",
    "    with open(filename, 'r',encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 의사 정보 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_doctor_profiles(max_pages, start_page=0):\n",
    "    base_url = 'https://kin.naver.com/people/expert/index.naver?type=DOCTOR&page={}'\n",
    "    doctor_info = []\n",
    "\n",
    "    for page in tqdm(range(start_page, max_pages + 1)):\n",
    "        url = base_url.format(page)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 각 의사의 상세 페이지 링크와 정보를 찾아 리스트에 추가\n",
    "        for index, item in enumerate(soup.select('.pro_list li'), start=1):\n",
    "            doctor_link_tag = item.find('h5').find('a')\n",
    "            if doctor_link_tag:\n",
    "                doctor_name = doctor_link_tag.text.strip()  # 닥터 이름 추출\n",
    "                doctor_id = doctor_link_tag['href'].split('u=')[1]  # 사용자 ID 추출\n",
    "                specialty_tag = item.find('h6')\n",
    "                specialty = specialty_tag.text.strip() if specialty_tag else '정보 없음' # 전문과목 추출\n",
    "                affiliation_tag = item.find('th', string='소속기관')\n",
    "                affiliation = affiliation_tag.find_next('td').text.strip() if affiliation_tag else '정보 없음' # 소속기관 추출\n",
    "                answer_count_tag = item.find('th', string='총 답변')\n",
    "                answer_count = int(answer_count_tag.find_next('td').text.strip().replace(',', '')) if answer_count_tag else 0 # 총 답변 수 추출\n",
    "\n",
    "                # 의사 정보를 딕셔너리로 저장\n",
    "                doctor_info.append({\n",
    "                    'index': index,\n",
    "                    'doctor_id': doctor_id,\n",
    "                    'doctor_name': doctor_name,\n",
    "                    'specialty': specialty,\n",
    "                    'total_answers': answer_count,\n",
    "                    'affiliation': affiliation\n",
    "                })\n",
    "\n",
    "    return doctor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 결과값 예시\n",
    "# doctor_profiles = scrape_doctor_profiles(1)\n",
    "# doctor_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 질문 id 크롤링(의사별)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info(doctor_id, total_answers, index):\n",
    "    max_pages = total_answers // 20 + (1 if total_answers % 20 else 0)\n",
    "    base_url = 'https://kin.naver.com/userinfo/expert/answerList.naver?u={user_id}&page={page}'\n",
    "    all_info = []\n",
    "\n",
    "    for page in tqdm(range(1, max_pages + 1)):\n",
    "        url = base_url.format(user_id=doctor_id, page=page)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # 정규 표현식으로 doc_id 추출\n",
    "        pattern = r'<a href=\"/qna/detail\\.naver\\?d1id=\\d+&dirId=\\d+&docId=(\\d+)\"'\n",
    "        matches = re.findall(pattern, response.text)\n",
    "\n",
    "        # 날짜 데이터 추출\n",
    "        dates = [date.text.strip() for date in soup.select('.t_num.tc')]\n",
    "        \n",
    "        for i, doc_id in enumerate(matches):\n",
    "            date = dates[i] if i < len(dates) else '날짜 없음'\n",
    "            all_info.append({'doc_id': doc_id, 'date': date})\n",
    "        \n",
    "    # 모든 페이지 정보가 포함된 하나의 JSON 파일 저장\n",
    "    folder_name = 'doc_id_data'\n",
    "    os.makedirs(folder_name, exist_ok=True) # 폴더가 없으면 생성\n",
    "    filename = os.path.join(folder_name, f'{index}_{doctor_id}.json')\n",
    "\n",
    "    save_to_json(all_info, filename)\n",
    "\n",
    "    return all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_ids = scrape_info(doctor_profiles[1]['doctor_id'],doctor_profiles[1]['total_answers'])\n",
    "# doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질문 답변 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일에 데이터를 추가하는 함수\n",
    "def append_to_csv(data, file_path):\n",
    "    # 파일이 존재하지 않으면 헤더를 포함하여 새 파일을 생성, 이미 존재하면 데이터만 추가\n",
    "    if not os.path.isfile(file_path):\n",
    "        df = pd.DataFrame([data])\n",
    "        df.to_csv(file_path, mode='w', index=False, encoding='utf-8-sig')\n",
    "    else:\n",
    "        df = pd.DataFrame([data])\n",
    "        df.to_csv(file_path, mode='a', index=False, encoding='utf-8-sig', header=False)\n",
    "\n",
    "# 스크래핑 함수 정의\n",
    "def scrape_details(doc_id):\n",
    "    base_url = 'https://kin.naver.com/qna/detail.naver?d1id=7&dirId=70201&docId={}'\n",
    "    headers = {\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7,ja;q=0.6',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "    }\n",
    "    try:\n",
    "        url = base_url.format(doc_id)\n",
    "        response = requests.get(url,timeout=5)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        data = {\n",
    "            'doc_id': doc_id,\n",
    "            'title': soup.select_one('.title').text.strip() if soup.select_one('.title') else 'N/A',\n",
    "            'question': soup.select_one('.c-heading__content').text.strip() if soup.select_one('.c-heading__content') else (soup.select_one('.c-heading__title').text.strip() if soup.select_one('.c-heading__title') else 'N/A'),\n",
    "            'answer': soup.select_one('.se-main-container').text.strip() if soup.select_one('.se-main-container') else (soup.select_one('._endContentsText.c-heading-answer__content-user').text.strip() if soup.select_one('._endContentsText.c-heading-answer__content-user') else 'N/A'),\n",
    "        }\n",
    "\n",
    "        # CSV 파일 경로 정의\n",
    "        csv_file_path = 'naver_QNA_details.csv'\n",
    "        # CSV 파일에 데이터 추가\n",
    "        append_to_csv(data, csv_file_path)\n",
    "\n",
    "    except Timeout:\n",
    "        print(f\"경고: {doc_id}에 대한 처리가 {5}초 동안 시간 초과되었습니다. 다음 반복으로 넘어갑니다.\")\n",
    "        timeout_doc_ids.append(doc_id)\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping doc_id {doc_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeout_doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⭐️모든 doc_id 추출⭐️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_list = []\n",
    "for filename in os.listdir('doc_id_data'):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join('doc_id_data', filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            doc_ids_list.extend(item['doc_id'] for item in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#여기서 각자 맡은 번호 주석 제거해서 실행하면됨\n",
    "'''\n",
    "0,1,2 민석\n",
    "\n",
    "3,4,5 민수\n",
    "\n",
    "6,7,8 승표\n",
    "\n",
    "9,10,11 성철\n",
    "'''\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 0\n",
    "# end = len(doc_ids_list)//12 * 1\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 1\n",
    "# end = len(doc_ids_list)//12 * 2\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 2\n",
    "# end = len(doc_ids_list)//12 * 3\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 3\n",
    "# end = len(doc_ids_list)//12 * 4\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 4\n",
    "# end = len(doc_ids_list)//12 * 5\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 5\n",
    "# end = len(doc_ids_list)//12 * 6\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 6\n",
    "# end = len(doc_ids_list)//12 * 7\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 7\n",
    "# end = len(doc_ids_list)//12 * 8 \n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 8\n",
    "# end = len(doc_ids_list)//12 * 9\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 9\n",
    "# end = len(doc_ids_list)//12 * 10\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 10\n",
    "# end = len(doc_ids_list)//12 * 11\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "# start = len(doc_ids_list)//12 * 11\n",
    "# end = len(doc_ids_list)//12 * 12 + len(doc_ids_list)%12\n",
    "# doc_ids_list= doc_ids_list[start:end]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_ids_list)\n",
    "last_saved = -1\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = last_saved + start + 1  # 이건 start 인덱스 알려주는거 한번 돌리다가 끊었을때는 이 코드를 사용해서 start 지정하면됨\n",
    "\n",
    "last_saved = start - 1  # 마지막으로 저장된 doc_id의 인덱스 초기화\n",
    "doc_ids_list = doc_ids_list[start:]\n",
    "try:\n",
    "    for i, doc_id in enumerate(tqdm(doc_ids_list)):\n",
    "        scrape_details(doc_id)\n",
    "        last_saved = i  # 성공적으로 처리될 때마다 업데이트\n",
    "except Exception as e:\n",
    "    print(f\"{last_saved}번까지 저장되었습니다. 오류: {e}\")\n",
    "else:\n",
    "    print(f\"{last_saved}번까지 저장되었습니다. 상세 데이터가 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복 제거 / 미작업 doc_id 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CSV 파일 불러오기\n",
    "# df = pd.read_csv('naver_QNA_details.csv')\n",
    "\n",
    "# # 'doc_id' 기준으로 중복 로우 제거, 첫 번째 출현을 유지\n",
    "# df_unique = df.drop_duplicates(subset='doc_id', keep='first')\n",
    "\n",
    "# # 중복 제거된 데이터프레임을 다시 CSV 파일로 저장\n",
    "# df_unique.to_csv('naver_QNA_details_unique.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CSV 파일에서 doc_id 열을 불러옴\n",
    "# df = pd.read_csv('naver_QNA_details_unique.csv')\n",
    "# completed_doc_ids = df['doc_id'].tolist()\n",
    "\n",
    "# # 작업하지 않은 doc_id만 찾기\n",
    "# remaining_doc_ids = list(set(doc_ids_list) - set(completed_doc_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
